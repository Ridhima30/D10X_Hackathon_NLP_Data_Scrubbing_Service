{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "D10X Hackathon - NLP Data Scrubbing Service",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdh9L0CReOXU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d9af636-15ac-4cb9-9a93-d5e46023cef7"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk import tokenize\n",
        "\n",
        "def scrubCommentary(commentary):\n",
        "  commentarySubSummary_text = decontracted(commentary['subscriptionSummary'])\n",
        "  commentarySubSummary_list = tokenize.sent_tokenize(commentarySubSummary_text)\n",
        "  scrub_commentary_main(commentarySubSummary_list, commentarySubSummary_text)\n",
        "\n",
        "  for pdfCommentary in commentary['attachedPDFContent']:\n",
        "    pdfCommentary_text = decontracted(pdfCommentary)\n",
        "    pdfCommentary_list = tokenize.sent_tokenize(pdfCommentary_text)\n",
        "    scrub_commentary_main(pdfCommentary_list, pdfCommentary_text)\n",
        "\n",
        "  commentaryHtmlContent_text = decontracted(commentary['htmlContent'])  \n",
        "  commentaryHtmlContent_list = tokenize.sent_tokenize(commentaryHtmlContent_text)\n",
        "  scrub_commentary_main(commentaryHtmlContent_list, commentaryHtmlContent_text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VmVEE1CIeGGh"
      },
      "source": [
        "# import pandas as pd\n",
        "# import re\n",
        "# from extractFromExcel import extractFromExcel\n",
        "# from nltk import tokenize\n",
        "\n",
        "# # Khyati's code will get us commentary\n",
        "# commentary = {}\n",
        "# commentary = extractFromExcel(0)\n",
        "\n",
        "# scrubCommentary(commentary)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PodvsDzZhmwf"
      },
      "source": [
        "def getQList():\n",
        "  df = pd.ExcelFile('location/test.xlsx').parse('Sheet1') #you could add index_col=0 if there's an index\n",
        "  qlist=[]\n",
        "  qlist.append(df['A'])\n",
        "  return qlist"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4bt_AQBv-vyV",
        "outputId": "b71ca4d0-bf64-47da-f95f-40851938f2be"
      },
      "source": [
        "#HELPER FUNCTIONS\n",
        "\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk import tokenize\n",
        "!pip install sentence-transformers\n",
        "from sentence_transformers import SentenceTransformer\n",
        "model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
        "import numpy as np\n",
        "\n",
        "def decontracted(phrase):\n",
        "    # specific\n",
        "    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
        "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
        "\n",
        "    # general\n",
        "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
        "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
        "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
        "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
        "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
        "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
        "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
        "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
        "    return phrase\n",
        "\n",
        "def check(word, x):\n",
        "    if word in x:\n",
        "        return True\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "def intersection(lst1, lst2): \n",
        "    return list(set(lst1) & set(lst2)) \n",
        "\n",
        "## GET INDEX\n",
        "def getIndexOfSentenceFromText(qElement, text):\n",
        "  sentences = tokenize.sent_tokenize(text)\n",
        "  print(sentences)\n",
        "  indexList = []\n",
        "  sentenceList = []\n",
        "  i = 0\n",
        "  for sentence in sentences:\n",
        "    if qElement in sentence:\n",
        "      ## CHECK FOR OUT OF BOUND INDEX IS PENDING, MIGHT NOT BE NEEDED\n",
        "      indexList.append(i)\n",
        "    i = i + 1\n",
        "  return indexList\n",
        "\n",
        "## SENTENCE SIMILARITY\n",
        "def cosine(u, v):\n",
        "    return np.dot(u, v) / (np.linalg.norm(u) * np.linalg.norm(v))\n",
        "\n",
        "def similarity(sent1, sent2):\n",
        "  query_vec = model.encode([sent1])[0]\n",
        "  sim = cosine(query_vec, model.encode([sent2])[0])\n",
        "  return sim"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "Collecting sentence-transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6a/e2/84d6acfcee2d83164149778a33b6bdd1a74e1bcb59b2b2cd1b861359b339/sentence-transformers-0.4.1.2.tar.gz (64kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 3.6MB/s \n",
            "\u001b[?25hCollecting transformers<5.0.0,>=3.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f9/54/5ca07ec9569d2f232f3166de5457b63943882f7950ddfcc887732fc7fb23/transformers-4.3.3-py3-none-any.whl (1.9MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9MB 6.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.41.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.7.1+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.19.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.4.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (3.2.5)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 22.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (3.7.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (20.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (3.0.12)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (2019.12.20)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/23/2ddc317b2121117bf34dd00f5b0de194158f2a44ee2bf5e47c7166878a97/tokenizers-0.10.1-cp37-cp37m-manylinux2010_x86_64.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 42.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (2.23.0)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 37.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->sentence-transformers) (3.7.4.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sentence-transformers) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->sentence-transformers) (1.15.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers<5.0.0,>=3.1.0->sentence-transformers) (3.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers<5.0.0,>=3.1.0->sentence-transformers) (2.4.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers) (3.0.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=3.1.0->sentence-transformers) (7.1.2)\n",
            "Building wheels for collected packages: sentence-transformers, sacremoses\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-0.4.1.2-cp37-none-any.whl size=103068 sha256=a2e6e9b7f27e62fad90e1e1c7ab35391f0d435ba5357597a5c6e5cfd06983dcf\n",
            "  Stored in directory: /root/.cache/pip/wheels/3d/33/d1/5703dd56199c09d4a1b41e0c07fb4e7765a84d787cbdc48ac3\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp37-none-any.whl size=893262 sha256=b92c98a3fd4e2d9d681ef4401c9884b11cecfec0087201c1a6fba86180ea3861\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sentence-transformers sacremoses\n",
            "Installing collected packages: tokenizers, sacremoses, transformers, sentencepiece, sentence-transformers\n",
            "Successfully installed sacremoses-0.0.43 sentence-transformers-0.4.1.2 sentencepiece-0.1.95 tokenizers-0.10.1 transformers-4.3.3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 405M/405M [00:21<00:00, 19.1MB/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "11GDVvQMyJfu",
        "outputId": "d6d5e6bb-5146-4970-9afb-be148d3a421b"
      },
      "source": [
        "!pip install spacy==2.1.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting spacy==2.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/78/0f/ca790def675011f25bce8775cf9002b5085cd2288f85e891f70b32c18752/spacy-2.1.0-cp37-cp37m-manylinux1_x86_64.whl (27.7MB)\n",
            "\u001b[K     |████████████████████████████████| 27.7MB 149kB/s \n",
            "\u001b[?25hCollecting blis<0.3.0,>=0.2.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fa/5f/47b7b29ad202b2210020e2f33bfb06d1db2abe0e709c2a84736e8a9d1bd5/blis-0.2.4-cp37-cp37m-manylinux1_x86_64.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 28.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.1.0) (1.19.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.0.12 in /usr/local/lib/python3.7/dist-packages (from spacy==2.1.0) (0.8.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy==2.1.0) (2.0.5)\n",
            "Requirement already satisfied: srsly<1.1.0,>=0.0.5 in /usr/local/lib/python3.7/dist-packages (from spacy==2.1.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.1.0) (1.0.5)\n",
            "Collecting preshed<2.1.0,>=2.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bc/2b/3ecd5d90d2d6fd39fbc520de7d80db5d74defdc2d7c2e15531d9cc3498c7/preshed-2.0.1-cp37-cp37m-manylinux1_x86_64.whl (82kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 8.4MB/s \n",
            "\u001b[?25hCollecting plac<1.0.0,>=0.9.6\n",
            "  Downloading https://files.pythonhosted.org/packages/9e/9b/62c60d2f5bc135d2aa1d8c8a86aaf84edb719a59c7f11a4316259e61a298/plac-0.9.6-py2.py3-none-any.whl\n",
            "Collecting thinc<7.1.0,>=7.0.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/36/42/d7ea7539af3852fd8c1f0b3adf4a100fb3d72b40b69cef1a764ff979a743/thinc-7.0.8-cp37-cp37m-manylinux1_x86_64.whl (2.1MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1MB 32.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.1.0) (2.23.0)\n",
            "Requirement already satisfied: jsonschema<3.0.0,>=2.6.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.1.0) (2.6.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.7/dist-packages (from thinc<7.1.0,>=7.0.2->spacy==2.1.0) (4.41.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.1.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.1.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.1.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.1.0) (2020.12.5)\n",
            "\u001b[31mERROR: en-core-web-sm 2.2.5 has requirement spacy>=2.2.2, but you'll have spacy 2.1.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: blis, preshed, plac, thinc, spacy\n",
            "  Found existing installation: blis 0.4.1\n",
            "    Uninstalling blis-0.4.1:\n",
            "      Successfully uninstalled blis-0.4.1\n",
            "  Found existing installation: preshed 3.0.5\n",
            "    Uninstalling preshed-3.0.5:\n",
            "      Successfully uninstalled preshed-3.0.5\n",
            "  Found existing installation: plac 1.1.3\n",
            "    Uninstalling plac-1.1.3:\n",
            "      Successfully uninstalled plac-1.1.3\n",
            "  Found existing installation: thinc 7.4.0\n",
            "    Uninstalling thinc-7.4.0:\n",
            "      Successfully uninstalled thinc-7.4.0\n",
            "  Found existing installation: spacy 2.2.4\n",
            "    Uninstalling spacy-2.2.4:\n",
            "      Successfully uninstalled spacy-2.2.4\n",
            "Successfully installed blis-0.2.4 plac-0.9.6 preshed-2.0.1 spacy-2.1.0 thinc-7.0.8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NCO5YBsW_qOH",
        "outputId": "b32ed6fe-28d6-4441-b956-0f4ad6cf7a3c"
      },
      "source": [
        "!pip install neuralcoref"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting neuralcoref\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/06/6d/c90e5bfd1b8ef32f1b231a32f2f625bf33df7525324d2bbcd08992791d64/neuralcoref-4.0-cp37-cp37m-manylinux1_x86_64.whl (286kB)\n",
            "\r\u001b[K     |█▏                              | 10kB 13.5MB/s eta 0:00:01\r\u001b[K     |██▎                             | 20kB 12.5MB/s eta 0:00:01\r\u001b[K     |███▍                            | 30kB 10.1MB/s eta 0:00:01\r\u001b[K     |████▋                           | 40kB 7.8MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 51kB 5.2MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 61kB 4.9MB/s eta 0:00:01\r\u001b[K     |████████                        | 71kB 5.5MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 81kB 5.9MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 92kB 5.9MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 102kB 5.2MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 112kB 5.2MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 122kB 5.2MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 133kB 5.2MB/s eta 0:00:01\r\u001b[K     |████████████████                | 143kB 5.2MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 153kB 5.2MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 163kB 5.2MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 174kB 5.2MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 184kB 5.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 194kB 5.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 204kB 5.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 215kB 5.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 225kB 5.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 235kB 5.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 245kB 5.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 256kB 5.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 266kB 5.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 276kB 5.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 286kB 5.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from neuralcoref) (2.23.0)\n",
            "Requirement already satisfied: spacy>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from neuralcoref) (2.1.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from neuralcoref) (1.19.5)\n",
            "Collecting boto3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b7/ae/09dd9c87e21973c00aea9a73e9862abf755c744d1f47e146bc5b8e08b6c2/boto3-1.17.20-py2.py3-none-any.whl (130kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 9.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->neuralcoref) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->neuralcoref) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->neuralcoref) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->neuralcoref) (1.24.3)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->neuralcoref) (2.0.5)\n",
            "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->neuralcoref) (0.9.6)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.0.12 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->neuralcoref) (0.8.2)\n",
            "Requirement already satisfied: srsly<1.1.0,>=0.0.5 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->neuralcoref) (1.0.5)\n",
            "Requirement already satisfied: thinc<7.1.0,>=7.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->neuralcoref) (7.0.8)\n",
            "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->neuralcoref) (2.0.1)\n",
            "Requirement already satisfied: blis<0.3.0,>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->neuralcoref) (0.2.4)\n",
            "Requirement already satisfied: jsonschema<3.0.0,>=2.6.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->neuralcoref) (2.6.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->neuralcoref) (1.0.5)\n",
            "Collecting botocore<1.21.0,>=1.20.20\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e4/c8/9336724456f158918f8101a9889e8dd6f709494e88820b99d8c410008b7c/botocore-1.20.20-py2.py3-none-any.whl (7.3MB)\n",
            "\u001b[K     |████████████████████████████████| 7.3MB 9.4MB/s \n",
            "\u001b[?25hCollecting s3transfer<0.4.0,>=0.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ea/43/4b4a1b26eb03a429a4c37ca7fdf369d938bd60018fc194e94b8379b0c77c/s3transfer-0.3.4-py2.py3-none-any.whl (69kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 6.1MB/s \n",
            "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading https://files.pythonhosted.org/packages/07/cb/5f001272b6faeb23c1c9e0acc04d48eaaf5c862c17709d20e3469c6e0139/jmespath-0.10.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.7/dist-packages (from thinc<7.1.0,>=7.0.2->spacy>=2.1.0->neuralcoref) (4.41.1)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.21.0,>=1.20.20->boto3->neuralcoref) (2.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.21.0,>=1.20.20->boto3->neuralcoref) (1.15.0)\n",
            "\u001b[31mERROR: botocore 1.20.20 has requirement urllib3<1.27,>=1.25.4, but you'll have urllib3 1.24.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: jmespath, botocore, s3transfer, boto3, neuralcoref\n",
            "Successfully installed boto3-1.17.20 botocore-1.20.20 jmespath-0.10.0 neuralcoref-4.0 s3transfer-0.3.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wwOhzoB_AlGu",
        "outputId": "7e048044-40ea-419f-d9ef-ae0f85986e57"
      },
      "source": [
        "!pip install https://github.com/explosion/spacy-models/releases//download/en_core_web_lg-2.1.0/en_core_web_lg-2.1.0.tar.gz"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting https://github.com/explosion/spacy-models/releases//download/en_core_web_lg-2.1.0/en_core_web_lg-2.1.0.tar.gz\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases//download/en_core_web_lg-2.1.0/en_core_web_lg-2.1.0.tar.gz (826.9MB)\n",
            "\u001b[K     |████████████████████████████████| 826.9MB 20kB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from en-core-web-lg==2.1.0) (2.1.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.0.12 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->en-core-web-lg==2.1.0) (0.8.2)\n",
            "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->en-core-web-lg==2.1.0) (2.0.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->en-core-web-lg==2.1.0) (2.0.5)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->en-core-web-lg==2.1.0) (1.19.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->en-core-web-lg==2.1.0) (2.23.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=0.0.5 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->en-core-web-lg==2.1.0) (1.0.5)\n",
            "Requirement already satisfied: thinc<7.1.0,>=7.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->en-core-web-lg==2.1.0) (7.0.8)\n",
            "Requirement already satisfied: blis<0.3.0,>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->en-core-web-lg==2.1.0) (0.2.4)\n",
            "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->en-core-web-lg==2.1.0) (0.9.6)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->en-core-web-lg==2.1.0) (1.0.5)\n",
            "Requirement already satisfied: jsonschema<3.0.0,>=2.6.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->en-core-web-lg==2.1.0) (2.6.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.1.0->en-core-web-lg==2.1.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.1.0->en-core-web-lg==2.1.0) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.1.0->en-core-web-lg==2.1.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.1.0->en-core-web-lg==2.1.0) (1.24.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.7/dist-packages (from thinc<7.1.0,>=7.0.2->spacy>=2.1.0->en-core-web-lg==2.1.0) (4.41.1)\n",
            "Building wheels for collected packages: en-core-web-lg\n",
            "  Building wheel for en-core-web-lg (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for en-core-web-lg: filename=en_core_web_lg-2.1.0-cp37-none-any.whl size=828255077 sha256=f77a5cc147f05684522bf448accb1fc836f5965c4055ef5ba38a8a3301a4f86d\n",
            "  Stored in directory: /root/.cache/pip/wheels/f5/23/48/c3271dd3a62b4dbe0edc676eca71ca861cf8d985675438d3dc\n",
            "Successfully built en-core-web-lg\n",
            "Installing collected packages: en-core-web-lg\n",
            "Successfully installed en-core-web-lg-2.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jiUxahM8j-En"
      },
      "source": [
        "def strike(text):\n",
        "    result = '\\u0336' + '\\u0336'.join(text)\n",
        "    return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AD2O2NOo-_Mq"
      },
      "source": [
        "def scrub_commentary_main(commentary, commentary_text):\n",
        "  # !pip install spacy==2.2.4\n",
        "  # !pip install neuralcoref\n",
        "\n",
        "  import spacy\n",
        "  from spacy import displacy\n",
        "  from collections import Counter\n",
        "  import en_core_web_lg\n",
        "\n",
        "  nlp = en_core_web_lg.load()\n",
        "  doc = nlp(commentary_text) \n",
        "\n",
        "  org_list=[]\n",
        "\n",
        "  for ent in doc.ents:\n",
        "    if ent.label_=='ORG':\n",
        "      org_list.append(ent.text)\n",
        "\n",
        "  print(\"Org List in Commentary -------------------- \",org_list)\n",
        "\n",
        "  qlist = ['Euroclear Bank']\n",
        "  #qlist = getQList()\n",
        "  final_qlist=intersection(qlist, org_list)\n",
        "  print(\"Final Q list is -------------------- \",final_qlist )\n",
        "  import neuralcoref\n",
        "\n",
        "  nlp = en_core_web_lg.load()\n",
        "  neuralcoref.add_to_pipe(nlp)\n",
        "  doc = nlp(commentary_text)\n",
        "  qword_ticker = {'Euroclear Bank':'Euroclear'}\n",
        "\n",
        "  for word in final_qlist:\n",
        "    print('%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%')\n",
        "    print(\"NEXT ITERATION HAS STARTED FOR THE QWORD -------------------- \", word)\n",
        "    \n",
        "    commentary= commentary_text.split('.')\n",
        "    print(\"New commentary list is =============== \", commentary)\n",
        "    commentary_len=len(commentary)\n",
        "    qword=word\n",
        "    print(\"Analyzing the Q Word --------------------\", qword)\n",
        "\n",
        "    qword_sent_idx=getIndexOfSentenceFromText(qword, commentary_text)\n",
        "    print(\"Q word sentence indices are --------------------\", qword_sent_idx)\n",
        "\n",
        "    relevant_sent_idx=[]\n",
        "    threshold=0.5\n",
        "  \n",
        "    for idx in qword_sent_idx:\n",
        "      if (idx not in relevant_sent_idx):\n",
        "        relevant_sent_idx.append(idx)\n",
        "      if idx-1>=0 and (idx-1 not in relevant_sent_idx) and similarity(commentary[idx], commentary[idx-1])>=threshold:\n",
        "        relevant_sent_idx.append(idx-1)\n",
        "      if idx-2>=0 and (idx-2 not in relevant_sent_idx) and similarity(commentary[idx], commentary[idx-2])>=threshold:\n",
        "        relevant_sent_idx.append(idx-2)\n",
        "      if idx+1<commentary_len and (idx+1 not in relevant_sent_idx) and similarity(commentary[idx], commentary[idx+1])>=threshold:\n",
        "        relevant_sent_idx.append(idx+1)\n",
        "      if idx+2<commentary_len and (idx+2 not in relevant_sent_idx) and similarity(commentary[idx], commentary[idx+2])>=threshold:\n",
        "        relevant_sent_idx.append(idx+2)\n",
        "    print(\"Relevant Sentence Index -------------------- \", relevant_sent_idx)\n",
        "\n",
        "    relevant_text=\"\"\n",
        "\n",
        "    for idx in relevant_sent_idx:\n",
        "      relevant_text = relevant_text+'. ' + commentary[idx]\n",
        "      \n",
        "    doc = nlp(relevant_text)\n",
        "\n",
        "    qword_cluster_list=[]\n",
        "    print(\"Clusters for the current commentary -------------------- \", doc._.coref_clusters)\n",
        "    for i in range(0 ,len(doc._.coref_clusters)):\n",
        "      if doc._.coref_clusters[i][0].text==qword:\n",
        "        print(doc._.coref_clusters[i].mentions)\n",
        "        \n",
        "        for word in doc._.coref_clusters[i].mentions:\n",
        "          qword_cluster_list.append(word.text)\n",
        "        print('Q Word Cluster List is --------------------', qword_cluster_list)\n",
        "          \n",
        "    scrub_list_idx=[]\n",
        "    for idx in relevant_sent_idx:\n",
        "      sentence=commentary[idx]\n",
        "      split_on_word=sentence.split(' ')\n",
        "      print(split_on_word)\n",
        "      for word in split_on_word:\n",
        "        if check(word, qword_cluster_list) or word==qword or word==qword_ticker[qword]:\n",
        "          scrub_list_idx.append(idx)\n",
        "          print('Scrubbed at word -------------------- ', word)\n",
        "          break\n",
        "        \n",
        "    new_commentary=\"\"\n",
        "    scrubbed_text=\"\"\n",
        "    \n",
        "\n",
        "    for idx in range(0,commentary_len):\n",
        "      if idx not in scrub_list_idx:\n",
        "        new_commentary = new_commentary+'. ' + commentary[idx]\n",
        "        scrubbed_text = scrubbed_text+'. '  + commentary[idx]\n",
        "      else:\n",
        "        scrubbed_text = scrubbed_text+'. '  + strike(commentary[idx])\n",
        "    \n",
        "    print(\"Original Text is -------------------- \", commentary)\n",
        "    print(\"Scrubbed Text is -------------------- \", scrubbed_text)\n",
        "      \n",
        "    commentary_text = new_commentary\n",
        "\n",
        "    print(\"New commentary text is -------------------- \", commentary_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NLF3592Y-mP1"
      },
      "source": [
        "commentary='Euroclear Bank informs customers that effective 09;30 Eastern Standard Time 11 January 2021 by means of the US Executive Order 13959 transactions involving the purchase by US persons of securities issued by companies on the Non-SDN Communist Chinese Military Companies List published by the Office of Foreign Assets Control as well as securities which are derivatives of, or providing an economic exposure to, such OFAC-listed securities will be prohibited. Disinvestment from such securities by U.S. persons to non U.S. persons is, however, permitted untill 23;59 on november 2021. For four additionalcompanies, listed on 31st December 2020, the prohibition will become effective on 1 February 2021 and the wind down to divest will be 2 December 2021.'\n",
        "commentary_text='Euroclear Bank informs customers that effective 09;30 Eastern Standard Time 11 January 2021 by means of the US Executive Order 13959 transactions involving the purchase by US persons of securities issued by companies on the Non-SDN Communist Chinese Military Companies List published by the Office of Foreign Assets Control as well as securities which are derivatives of, or providing an economic exposure to, such OFAC-listed securities will be prohibited. Disinvestment from such securities by U.S. persons to non U.S. persons is, however, permitted untill 23;59 on november 2021. For four additionalcompanies, listed on 31st December 2020, the prohibition will become effective on 1 February 2021 and the wind down to divest will be 2 December 2021.'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QA4zsSWWJ7g2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70bdedb2-69e5-4829-c2d0-efff45e54eb4"
      },
      "source": [
        "print(commentary)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Euroclear Bank informs customers that effective 09;30 Eastern Standard Time 11 January 2021 by means of the US Executive Order 13959 transactions involving the purchase by US persons of securities issued by companies on the Non-SDN Communist Chinese Military Companies List published by the Office of Foreign Assets Control as well as securities which are derivatives of, or providing an economic exposure to, such OFAC-listed securities will be prohibited. Disinvestment from such securities by U.S. persons to non U.S. persons is, however, permitted untill 23;59 on november 2021. For four additionalcompanies, listed on 31st December 2020, the prohibition will become effective on 1 February 2021 and the wind down to divest will be 2 December 2021.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vKB5uDcQlvic",
        "outputId": "25e431c7-b0dd-4cf8-915c-b143d22f87a6"
      },
      "source": [
        "scrub_commentary_main(tokenize.sent_tokenize(decontracted(commentary)), decontracted(commentary_text))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Org List in Commentary --------------------  ['Euroclear Bank', 'Eastern Standard Time', 'the US', 'the Non-SDN Communist Chinese Military Companies List', 'the Office of Foreign Assets Control', 'OFAC']\n",
            "Final Q list is --------------------  ['Euroclear Bank']\n",
            "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
            "NEXT ITERATION HAS STARTED FOR THE QWORD --------------------  Euroclear Bank\n",
            "New commentary list is ===============  ['Euroclear Bank informs customers that effective 09;30 Eastern Standard Time 11 January 2021 by means of the US Executive Order 13959 transactions involving the purchase by US persons of securities issued by companies on the Non-SDN Communist Chinese Military Companies List published by the Office of Foreign Assets Control as well as securities which are derivatives of, or providing an economic exposure to, such OFAC-listed securities will be prohibited', ' Disinvestment from such securities by U', 'S', ' persons to non U', 'S', ' persons is, however, permitted untill 23;59 on november 2021', ' For four additionalcompanies, listed on 31st December 2020, the prohibition will become effective on 1 February 2021 and the wind down to divest will be 2 December 2021', '']\n",
            "Analyzing the Q Word -------------------- Euroclear Bank\n",
            "['Euroclear Bank informs customers that effective 09;30 Eastern Standard Time 11 January 2021 by means of the US Executive Order 13959 transactions involving the purchase by US persons of securities issued by companies on the Non-SDN Communist Chinese Military Companies List published by the Office of Foreign Assets Control as well as securities which are derivatives of, or providing an economic exposure to, such OFAC-listed securities will be prohibited.', 'Disinvestment from such securities by U.S. persons to non U.S. persons is, however, permitted untill 23;59 on november 2021.', 'For four additionalcompanies, listed on 31st December 2020, the prohibition will become effective on 1 February 2021 and the wind down to divest will be 2 December 2021.']\n",
            "Q word sentence indices are -------------------- [0]\n",
            "Relevant Sentence Index --------------------  [0, 1]\n",
            "Clusters for the current commentary --------------------  []\n",
            "['Euroclear', 'Bank', 'informs', 'customers', 'that', 'effective', '09;30', 'Eastern', 'Standard', 'Time', '11', 'January', '2021', 'by', 'means', 'of', 'the', 'US', 'Executive', 'Order', '13959', 'transactions', 'involving', 'the', 'purchase', 'by', 'US', 'persons', 'of', 'securities', 'issued', 'by', 'companies', 'on', 'the', 'Non-SDN', 'Communist', 'Chinese', 'Military', 'Companies', 'List', 'published', 'by', 'the', 'Office', 'of', 'Foreign', 'Assets', 'Control', 'as', 'well', 'as', 'securities', 'which', 'are', 'derivatives', 'of,', 'or', 'providing', 'an', 'economic', 'exposure', 'to,', 'such', 'OFAC-listed', 'securities', 'will', 'be', 'prohibited']\n",
            "Scrubbed at word --------------------  Euroclear\n",
            "['', 'Disinvestment', 'from', 'such', 'securities', 'by', 'U']\n",
            "Original Text is --------------------  ['Euroclear Bank informs customers that effective 09;30 Eastern Standard Time 11 January 2021 by means of the US Executive Order 13959 transactions involving the purchase by US persons of securities issued by companies on the Non-SDN Communist Chinese Military Companies List published by the Office of Foreign Assets Control as well as securities which are derivatives of, or providing an economic exposure to, such OFAC-listed securities will be prohibited', ' Disinvestment from such securities by U', 'S', ' persons to non U', 'S', ' persons is, however, permitted untill 23;59 on november 2021', ' For four additionalcompanies, listed on 31st December 2020, the prohibition will become effective on 1 February 2021 and the wind down to divest will be 2 December 2021', '']\n",
            "Scrubbed Text is --------------------  . ̶E̶u̶r̶o̶c̶l̶e̶a̶r̶ ̶B̶a̶n̶k̶ ̶i̶n̶f̶o̶r̶m̶s̶ ̶c̶u̶s̶t̶o̶m̶e̶r̶s̶ ̶t̶h̶a̶t̶ ̶e̶f̶f̶e̶c̶t̶i̶v̶e̶ ̶0̶9̶;̶3̶0̶ ̶E̶a̶s̶t̶e̶r̶n̶ ̶S̶t̶a̶n̶d̶a̶r̶d̶ ̶T̶i̶m̶e̶ ̶1̶1̶ ̶J̶a̶n̶u̶a̶r̶y̶ ̶2̶0̶2̶1̶ ̶b̶y̶ ̶m̶e̶a̶n̶s̶ ̶o̶f̶ ̶t̶h̶e̶ ̶U̶S̶ ̶E̶x̶e̶c̶u̶t̶i̶v̶e̶ ̶O̶r̶d̶e̶r̶ ̶1̶3̶9̶5̶9̶ ̶t̶r̶a̶n̶s̶a̶c̶t̶i̶o̶n̶s̶ ̶i̶n̶v̶o̶l̶v̶i̶n̶g̶ ̶t̶h̶e̶ ̶p̶u̶r̶c̶h̶a̶s̶e̶ ̶b̶y̶ ̶U̶S̶ ̶p̶e̶r̶s̶o̶n̶s̶ ̶o̶f̶ ̶s̶e̶c̶u̶r̶i̶t̶i̶e̶s̶ ̶i̶s̶s̶u̶e̶d̶ ̶b̶y̶ ̶c̶o̶m̶p̶a̶n̶i̶e̶s̶ ̶o̶n̶ ̶t̶h̶e̶ ̶N̶o̶n̶-̶S̶D̶N̶ ̶C̶o̶m̶m̶u̶n̶i̶s̶t̶ ̶C̶h̶i̶n̶e̶s̶e̶ ̶M̶i̶l̶i̶t̶a̶r̶y̶ ̶C̶o̶m̶p̶a̶n̶i̶e̶s̶ ̶L̶i̶s̶t̶ ̶p̶u̶b̶l̶i̶s̶h̶e̶d̶ ̶b̶y̶ ̶t̶h̶e̶ ̶O̶f̶f̶i̶c̶e̶ ̶o̶f̶ ̶F̶o̶r̶e̶i̶g̶n̶ ̶A̶s̶s̶e̶t̶s̶ ̶C̶o̶n̶t̶r̶o̶l̶ ̶a̶s̶ ̶w̶e̶l̶l̶ ̶a̶s̶ ̶s̶e̶c̶u̶r̶i̶t̶i̶e̶s̶ ̶w̶h̶i̶c̶h̶ ̶a̶r̶e̶ ̶d̶e̶r̶i̶v̶a̶t̶i̶v̶e̶s̶ ̶o̶f̶,̶ ̶o̶r̶ ̶p̶r̶o̶v̶i̶d̶i̶n̶g̶ ̶a̶n̶ ̶e̶c̶o̶n̶o̶m̶i̶c̶ ̶e̶x̶p̶o̶s̶u̶r̶e̶ ̶t̶o̶,̶ ̶s̶u̶c̶h̶ ̶O̶F̶A̶C̶-̶l̶i̶s̶t̶e̶d̶ ̶s̶e̶c̶u̶r̶i̶t̶i̶e̶s̶ ̶w̶i̶l̶l̶ ̶b̶e̶ ̶p̶r̶o̶h̶i̶b̶i̶t̶e̶d.  Disinvestment from such securities by U. S.  persons to non U. S.  persons is, however, permitted untill 23;59 on november 2021.  For four additionalcompanies, listed on 31st December 2020, the prohibition will become effective on 1 February 2021 and the wind down to divest will be 2 December 2021. \n",
            "New commentary text is --------------------  .  Disinvestment from such securities by U. S.  persons to non U. S.  persons is, however, permitted untill 23;59 on november 2021.  For four additionalcompanies, listed on 31st December 2020, the prohibition will become effective on 1 February 2021 and the wind down to divest will be 2 December 2021. \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hW9Izkikw4NI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aH3Fo41OqF3S"
      },
      "source": [
        "import re\n",
        "alphabets= \"([A-Za-z])\"\n",
        "prefixes = \"(Mr|St|Mrs|Ms|Dr|Prof|Capt|Cpt|Lt|Mt)[.]\"\n",
        "websites = \"[.](com|net|org|io|gov|me|edu)\"\n",
        "suffixes = \"(Inc|Ltd|Jr|Sr|Co)\"\n",
        "starters = \"(Mr|Mrs|Ms|Dr|He\\s|She\\s|It\\s|They\\s|Their\\s|Our\\s|We\\s|But\\s|However\\s|That\\s|This\\s|Wherever)\"\n",
        "acronyms = \"([A-Z][.][A-Z][.](?:[A-Z][.])?)\"\n",
        "\n",
        "def split_into_sentences(text):\n",
        "    text = \" \" + text + \"  \"\n",
        "    text = text.replace(\"\\n\",\" \")\n",
        "    text = re.sub(prefixes,\"\\\\1<prd>\",text)\n",
        "    text = re.sub(websites,\"<prd>\\\\1\",text)\n",
        "    if \"Ph.D\" in text: text = text.replace(\"Ph.D.\",\"Ph<prd>D<prd>\")\n",
        "    text = re.sub(\"\\s\" + alphabets + \"[.] \",\" \\\\1<prd> \",text)\n",
        "    text = re.sub(acronyms+\" \"+starters,\"\\\\1<stop> \\\\2\",text)\n",
        "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\\\\3<prd>\",text)\n",
        "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\",text)\n",
        "    text = re.sub(\" \"+suffixes+\"[.] \"+starters,\" \\\\1<stop> \\\\2\",text)\n",
        "    text = re.sub(\" \"+suffixes+\"[.]\",\" \\\\1<prd>\",text)\n",
        "    text = re.sub(\" \" + alphabets + \"[.]\",\" \\\\1<prd>\",text)\n",
        "    if \"”\" in text: text = text.replace(\".”\",\"”.\")\n",
        "    if \"\\\"\" in text: text = text.replace(\".\\\"\",\"\\\".\")\n",
        "    if \"!\" in text: text = text.replace(\"!\\\"\",\"\\\"!\")\n",
        "    if \"?\" in text: text = text.replace(\"?\\\"\",\"\\\"?\")\n",
        "    text = text.replace(\".\",\".<stop>\")\n",
        "    text = text.replace(\"?\",\"?<stop>\")\n",
        "    text = text.replace(\"!\",\"!<stop>\")\n",
        "    text = text.replace(\"<prd>\",\".\")\n",
        "    sentences = text.split(\"<stop>\")\n",
        "    sentences = sentences[:-1]\n",
        "    sentences = [s.strip() for s in sentences]\n",
        "    return sentences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SffzA-pZqJAR",
        "outputId": "6053e6dd-8a6c-48bb-d0f0-178324c8fb0e"
      },
      "source": [
        "print(split_into_sentences(\"Mr. John Johnson Jr. was born in the U.S.A but earned his Ph.D. in Israel before joining Nike Inc. as an engineer. He also worked at craigslist.org as a business analyst.\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Mr. John Johnson Jr. was born in the U.S.A but earned his Ph.D. in Israel before joining Nike Inc. as an engineer.', 'He also worked at craigslist.org as a business analyst.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vxvs04EmqjLI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}